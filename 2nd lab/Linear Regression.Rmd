---
title: "Lab 2 - Parte 2: Regressão linear para explicar desempenho acadêmico"
author: "Luiz Fonseca"
date: "26 de novembro de 2016"
output: html_document
---
<hr>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# setwd("/home/luiz/Faculdade/Predictive-Data-Analysis/2nd lab/")
```

Primeiro vamos carregar os dados e bibliotecas que serão utilizados.

```{r, results='hide', message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(car)
library(leaps)

alunos.graduados <- read.csv("../data/graduados.csv")
```

## Entendendo o problema e os dados

Os dados são sobre alunos graduados do curso de Ciência da Computação da Universidade Federal de Campina Grande. Cada linha do data frame alunos.graduados representa o desempenho de um aluno em uma determinada disciplina. A tarefa é verificar se é possível prever, utilizando regressão linear múltipla, o desempenho final do aluno no curso (variável cra) a partir de seu desempenho nos dois primeiros semestres do curso.

## Tratando os dados

O arquivo original não está no formato ótimo para usármo-lo como entrada da função que gera um modelo linear. Abaixo, encontra-se o passo a passo para derivarmos um dataframe que podemos utilizar como entrada da função.

```{r, message=FALSE}
# Retira os valores NA das médias
alunos.graduados <- alunos.graduados %>% 
  arrange(matricula) %>%
  filter(!is.na(media))

# Calcula o CRA de cada aluno
graduados.cra <- alunos.graduados %>%
  group_by(matricula) %>%
  mutate(cra.contrb = media*creditos) %>%
  summarise(cra = round(sum(cra.contrb)/sum(creditos), 2))

disciplinas.iniciais <- c(
  "Cálculo Diferencial e Integral I",
  "Álgebra Vetorial e Geometria Analítica",
  "Leitura e Produção de Textos",
  "Programação I",
  "Introdução à Computação",
  "Laboratório de Programação I",
  "Cálculo Diferencial e Integral II",
  "Matemática Discreta",
  "Programação II",
  "Teoria dos Grafos",
  "Fundamentos de Física Clássica",
  "Laboratório de Programação II"
  )

# Transforma o dataframe em um formato ideal para ser utilizado como entrada do modelo
graduados.model.input <- alunos.graduados %>%
  filter(disciplina %in% disciplinas.iniciais) %>%
  group_by(matricula,disciplina)  %>%
  filter(media == max(media)) %>%
  ungroup() %>%
  select(matricula,disciplina,media) %>% 
  mutate(disciplina = as.factor(gsub(" ",".",disciplina))) %>%
  dcast(matricula ~ disciplina, mean) %>%
  merge(graduados.cra)
```

## Questionamentos
<ol>
<li> <b> Um modelo de regressão múltipla com todas as variáveis é plausível para explicar a variação em y? Em que grau? </b> </li>

Vamos criar um modelo de regressão múltipla e analisá-lo.

```{r}
cra.model.fit <- lm(formula = cra ~ . -matricula, graduados.model.input, na.action = na.omit)
summary(cra.model.fit)
```

Vamos analisar cada coeficiente e estatística do modelo para termos uma noção geral. A estatística F nos dá uma ideia sobre o teste de hipóteses feito para verificar se pelo menos um dos coeficientes das variáveis preditoras é diferente de 0, isto é, se há pelo menos uma variável preditora que está relacionada com a variável que se quer prevê que neste caso é o cra. Como o valor da estatística F é maior que 1 e o tamanho da amostra (n = 103) é consideravelmente maior que o número de preditores (p = 12), podemos inferir que há sim uma relação entre pelo menos uma das variáveis de entrada e a variável alvo. Isso também pode ser verificado olhando os coeficientes das variáveis de entrada, que são todos diferentes de 0.

Tendo verificado que as variáveis de entrada estão relacionadas com a variável alvo, agora podemos olhar para os p-valores de cada variável preditora. O p valor é uma probabilidade que nos fornece evidências para a hipótese nula. Um p-valor menor que 0,05 é considerado bom e indica que há fortes evidências de que a variável é uma boa preditora no modelo. Um p-valor maior que 0,05 é considerado alto e indica que talvez a variável não se adeque ao modelo.

No modelo gerado com todas as variáveis, a maioria dos p-valores das variáveis está acima do limite, que é 0,05. Isso indica que talvez seja necessário retirar algumas variáveis que não fazem sentido para o modelo de regressão.

Um coeficiente que merece destaque é o r² ajustado. Para o modelo gerado, o R² ajustado - que mede o quão bem o modelo explica a variabilidade dos dados com relação a média da variável observada - é cerca de 0.6473 e isso significa que o modelo consegue exlicar a maior parte dos dados. 
Esse valor de r² ajustado é satisfatório para o propósito desta atividade. Porém, como os p-valores das variáveis de predição estão muito altos, devemos considerar a retirada de algumas variáveis do modelo, de forma que possamos encontrar um equilíbrio entre o r² ajustado e os p-valores. Então, um modelo com todas as variáveis talvez não seja o melhor possível.
<hr>
<li><b>Todas as variáveis são úteis para o modelo de regressão?</b></li>

Para responder essa pergunta, iremos executar a tarefa conhecida como <i> seleção de variáveis </i>, que consiste em diagnosticar quais preditores estão associados com a resposta e quais podem ser descartados. Existem vários algoritmos de seleção de váriáveis: forward selection, backward selection, mixed selection, busca exaustiva, entre outros. Utilizarei o pacote leaps, que fornece métodos para executar vários tipos de seleção. O método que será utilizado é a busca exaustiva (exhaustive search), que irá procurar o melhor modelo dentre todos os possíveis.

```{r}
# library(leaps)
regsubsets.out <-
    regsubsets(cra ~ . -matricula,
               data = graduados.model.input,
               nbest = 1,       # 1 único modelo para cada cojunto de preditores
               nvmax = NULL,    # NULL para não haver limite no número de variáveis preditoras
               force.in = NULL, force.out = NULL,
               method = "exhaustive")

summary.out <- summary(regsubsets.out)
summary.out
```

Quando executamos summary(regsubsets.out), nos é mostrado o melhor modelo para cada tamanho de subconjunto de variáveis. Para este caso, a saída nos mostrará o melhor modelo utilizando apenas uma variável, o melhor modelo utilizando 2 variáveis e assim continua até todas as doze variáveis. O melhor modelo é definido em termos de maior R² ajustado e p-valores satisfatórios para as variáveis preditoras. Nos resta saber qual desses doze modelos representa o melhor modelo.

```{r}
# Qual dos doze modelos contém o maior r² ajustado
which.max(summary.out$adjr2)
# Quais variáveis estão presentes nesse modelo
summary.out$which[6,]
```

Com isso descobrimos que o melhor modelo é aquele que contem apenas seis variáveis preditoras. Então, a partir destas seis variáveis, definiremos um novo modelo. 

```{r}
graduados.best.input <- graduados.model.input %>%
  dplyr::select(
    Álgebra.Vetorial.e.Geometria.Analítica, Introdução.à.Computação,
    Leitura.e.Produção.de.Textos, Matemática.Discreta, Programação.II,
    Teoria.dos.Grafos, cra)

cra.best.fit <- lm(formula = cra ~ ., graduados.best.input)
summary(cra.best.fit)
```
<hr>
<li><b> Se a resposta para a pergunta anterior foi não, construa um novo modelo sem essas variáveis e o compare ao modelo com todas as variáveis (e.g. em termos de R2 e RSE). </b> </li>

Agora vamos comparar os dois modelos encontrados: o modelo com todas as variáveis e o modelo com seis variáveis apenas. O primeiro modelo possui maior r² ajustado, 0.6473 contra 0.6075, do segundo modelo. O primeiro modelo possui um RSE menor do que o segundo, 0.5046 contra 0.526. 

O r² é o quadrado da correlação entre a variável de resposta e o modelo linear ajustado - Cor(Y, Ŷ )² -  e mede a fração de variância explicada. Poderíamos pensar que o primeiro modelo é melhor porque seu r² é maior e, portanto, ele explicaria melhor a variabilidade dos dados, mas isso não é verdade. Acontece que o r² irá sempre aumentar ao incluir novas variáveis no modelo, mesmo aquelas que estão fracamente associadas com a variável de resposta, ou seja, possuem p-valor alto. Isso se deve ao fato de que adicionar uma variável às equações de mínimos quadrados nos permitirá ajustar os dados de treino (embora não necessariamente os dados de teste) mais acuradamente. Podemos perceber que a diferença do r² ajustado é muito pequeno de um modelo para o outro (apenas 0.0398), o que nos fornece evidências adicionais de que as seis variáveis retiradas do modelo não são boas preditoras.

O RSE é o <i>erro padrão dos desvios</i> ou <i>erro padrão da regressão</i>. Quanto menor o RSE, melhor é a previsão dada pelo modelo. Apesar do RSE do segundo modelo ser maior do que o RSE do primeiro, devemos observar também os graus de liberdade, que para o segundo modelo são 285 e para o primeiro são 90. Então, a pequena diferença entre os RSEs dos modelos pode ser relevada considerando o aumento de 4 vezes mais graus de liberdade.

Também poderíamos comparar os p-valores das variáveis preditoras em ambos os modelos. Assumindo que um p-valor satisfatório é menor que 0.05, verificamos que das doze variáveis preditoras utilizadas no modelo 1, apenas 2 possuem um p-valor aceitável. Em contrapartida, todas as seis variáveis do modelo 2 possuem p-valores muito abaixo do limite. Isso nos diz que no segundo modelo, todas as variáveis estão altamente relacionadas com a variável resposta, ao contrário do primeiro.
<hr>
<li><b> Analise os plots de resíduos de cada variável e veja se algum (um ou mais) deles indica não aleatoriedade dos erros. </li></b>

Abaixo, apresentamos os scatter plots dos resíduos para cada variável preditora.

```{r, fig.width=9, fig.height=6, echo=FALSE}
graduados.model.input <- na.omit(graduados.model.input)
graduados.model.input$residuals <- residuals(cra.model.fit)

variable.names <- c(names(graduados.model.input)[2:13], "residuals")
plotDF <- melt(graduados.model.input[, variable.names], id="residuals")

ggplot(plotDF, aes(x=value, y=residuals)) + 
  geom_point(color="slateblue") + facet_wrap(~ variable)
```

Analisando os plots dos resíduos de cada variável, identifica-se aleatoriedade em todos os gráficos e uma simetria dos pontos em torno do valor 0. Então, concluímos que nenhuma variável tende a superestimar ou subestimar a variável de resposta.
<hr>
<li><b> Que período consegue explicar melhor o desempenho final (primeiro ou segundo)? </li></b>

Para responder essa questão, farei um modelo otimizado para cada período, utilizando somente as disciplinas boas preditoras e depois irei comparar os modelos. Mas primeiramente vamos fazer dois modelos com todas as suas respectivas variáveis preditoras.

```{r}
model.input.semester.1 <- graduados.model.input %>%
  select(cra, matricula, Álgebra.Vetorial.e.Geometria.Analítica, Cálculo.Diferencial.e.Integral.I,
         Introdução.à.Computação, Laboratório.de.Programação.I, Programação.I, Leitura.e.Produção.de.Textos)

model.input.semester.2 <- graduados.model.input %>%
  select(cra, matricula, Cálculo.Diferencial.e.Integral.II, Fundamentos.de.Física.Clássica,
         Laboratório.de.Programação.II, Matemática.Discreta, Programação.II, Teoria.dos.Grafos)

model.semester.1 <- lm(cra ~ . -matricula, model.input.semester.1)
model.semester.2 <- lm(cra ~ . -matricula, model.input.semester.2)

summary(model.semester.1)
summary(model.semester.2)
```

Podemos verificar que ambos os modelos possuem variáveis preditoras com p-valores muito alto. Isto é, algumas variáveis não explicam bem o cra. Para podermos comparar melhor os modelos dos períodos vamos melhorá-los usando um dos algoritmos de seleção de variáveis.

```{r}
## Modelo para o 1º semestre
regsubsets.semester.1 <-
    regsubsets(cra ~ . -matricula,
               data = model.input.semester.1,
               nbest = 1,       # 1 único modelo para cada cojunto de preditores
               nvmax = NULL,    # NULL para não haver limite no número de variáveis preditoras
               force.in = NULL, force.out = NULL,
               method = "exhaustive")

# summary.semester.1 <- summary(regsubsets.semester.1)
# index1 <- which.max(summary.semester.1$adjr2)
# summary.semester.1$which[index1,]

# Vamos tirar as disciplinas Programação I e Cálculo I, pois sem estas duas já conseguimos um modelo satisfatório.
best.input.semester.1 <- model.input.semester.1 %>%
  select(-Programação.I, -Cálculo.Diferencial.e.Integral.I)
best.model.semester.1 <- lm(cra ~ . -matricula, best.input.semester.1)

## Modelo para o 2º semestre
regsubsets.semester.2 <-
    regsubsets(cra ~ . -matricula,
               data = model.input.semester.2,
               nbest = 1,       # 1 único modelo para cada cojunto de preditores
               nvmax = NULL,    # NULL para não haver limite no número de variáveis preditoras
               force.in = NULL, force.out = NULL,
               method = "exhaustive")

# summary.semester.2 <- summary(regsubsets.semester.2)
# index2 <- which.max(summary.semester.2$adjr2)
# summary.semester.2$which[index2,]

# Vamos tirar as disciplinas Física Clássica, Cálculo II, Laboratório de Programação II para otimizar o modelo.
best.input.semester.2 <- model.input.semester.2 %>%
  select(-Fundamentos.de.Física.Clássica, -Cálculo.Diferencial.e.Integral.II, -Laboratório.de.Programação.II)
best.model.semester.2 <- lm(cra ~ . -matricula, best.input.semester.2)

summary(best.model.semester.1)
summary(best.model.semester.2)
```

Comparando os dois modelos melhorados, temos que o modelo do primeiro período possui maior RSE e menor R² ajustado. Então, podemos tomar o modelo do segundo período como o melhor entre os dois. O modelo do segundo período, com apenas três variáveis preditores ainda se saiu melhor do que o modelo que inclui ambos os períodos em termos de r² e RSE. Vamos utilizar uma visualização para comparar os três modelos.

```{r, echo=FALSE}
comparing.models <- data.frame(modelo = c("1º período", "2º período", "ambos"),
                               r_quadrado_ajustado = c(0.4641, 0.6457, 0.6075),
                               RSE = c(0.6221, 0.5058, 0.526))

data.melted <- melt(comparing.models, id.vars='modelo')

ggplot(data.melted, aes(modelo, value)) +   
  geom_bar(aes(fill = variable), position = "dodge", stat="identity") +
  labs(title = "Comparação de modelos", x = "modelo", y = "valor", fill = "estatística")
```

Dentre os três, o modelo para o segundo período é o que possui maior r² ajustado e menor RSE. Podemos presumir que ele seja o modelo que melhor prevê o CRA.
<hr>
<li><b> Use o melhor modelo encontrado para predizer o seu próprio desempenho e compare a predição com o seu CRA atual. Comente o resultado. </li></b>

```{r}
notas.disciplinas <- data.frame(matricula = c(114110463),
                                Matemática.Discreta = c(9.4), 
                                Programação.II = c(8.8),
                                Teoria.dos.Grafos = c(10.0),
                                Álgebra.Vetorial.e.Geometria.Analítica = c(9.7),
                                Introdução.à.Computação = c(9.4),
                                Laboratório.de.Programação.I = c(8.0),
                                Leitura.e.Produção.de.Textos = c(8.6))

predicao.cra.1 <- round(predict(best.model.semester.1, notas.disciplinas), 2)
predicao.cra.2 <- round(predict(best.model.semester.2, notas.disciplinas), 2)
predicao.cra.ambos <- round(predict(cra.best.fit, notas.disciplinas), 2)
```

```{r, echo=FALSE}
cat(paste(" Predição do modelo do 1º semestre: ", predicao.cra.1, "\n",
          "Predição do modelo do 2º semestre: ", predicao.cra.2, "\n",
          "Predição do modelo de ambos os semestres: ", predicao.cra.ambos))
```

O resultado da previsão do modelo do segundo semestre, que havia sido o melhor entre os três, foi muito próxima do meu CRA atual. O meu CRA é 8,78 e o modelo previu 8.65. Como esperamos que o cra não sofra uma drástica mudança até o final do curso, podemos dizer que a previsão é bastante razoável, pelo menos para o meu caso. Seria necessário uma quantidade maior de dados de testes para termos certeza sobre a acurácia do modelo. 

Como esta é uma atividade introdutória de regressão linear múltipla e ajuste de modelos podemos nos conformar com este modelo final. Porém, para termos certeza da capacidade preditiva do modelo, precisaríamos aplicar outros melhoramentos e técnicas para ajustar melhor o modelo.

## Conclusão

É possível prever o resultado final dos alunos com base nos dois primeiros semestres. Com apenas poucos melhoramentos, conseguimos ajustar um modelo que explica cerca de 65% da variabilidade dos dados. 

<hr>
<hr>